import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

try:
    from thefuzz import fuzz
except ImportError:
    st.error("Hi√°nyzik: pip install thefuzz python-Levenshtein")
    st.stop()

import json
import io

# ============================================================================
# üé® HYPER App - Neuromarketing ROAS Predictor v3.0
# F√ÅZIS 1: CSV Importer & Intelligent Mapper
# ============================================================================

st.set_page_config(
    page_title="HYPER - Marketing Predictor",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# üìä CONFIGURATION & MAPPINGS
# ============================================================================

UNIFIED_SCHEMA = {
    "mandatory": [
        ("date_start", "date", "Jelent√©s kezdete (d√°tum)"),
        ("date_end", "date", "Jelent√©s v√©ge (d√°tum)"),
        ("campaign_name", "string", "Kamp√°ny neve"),
        ("platform", "string", "Platform (Facebook/Google Ads/TikTok)"),
        ("campaign_status", "string", "Kamp√°ny st√°tusza"),
        ("spend", "float", "Elk√∂lt√∂tt √∂sszeg (HUF)"),
        ("conversions", "int", "Konverzi√≥k / V√°s√°rl√°sok"),
        ("conversion_value", "float", "Konverzi√≥s √©rt√©k (HUF)"),
    ],
    "recommended": [
        ("impressions", "int", "Megjelen√©sek"),
        ("clicks", "int", "Kattint√°sok / Interakci√≥k"),
        ("ctr_percent", "float", "CTR (%)"),
        ("cpc", "float", "CPC (HUF)"),
        ("cpa", "float", "CPA (HUF)"),
        ("roas", "float", "ROAS"),
        ("reach", "int", "El√©r√©s"),
        ("frequency", "float", "Gyakoris√°g"),
        ("ad_group_name", "string", "Ad Set / Ad Group neve"),
        ("budget_type", "string", "K√∂lts√©gkeret t√≠pusa"),
        ("budget_allocated", "float", "K√∂lts√©gkeret (HUF)"),
    ],
    "optional": [
        ("add_to_cart", "int", "Kos√°rba helyez√©sek"),
        ("video_views", "int", "Vide√≥ megtekint√©sek"),
        ("engagement", "int", "Engagement"),
        ("conversion_type", "string", "Konverzi√≥ t√≠pusa"),
        ("notes", "string", "Megjegyz√©sek"),
    ]
}

# Fuzzy matching patterns
COLUMN_PATTERNS = {
    # Spend related
    "spend": ["elk√∂lt√∂tt", "k√∂lts√©g", "spend", "ad spend", "expense", "amount spent"],
    
    # Conversions
    "conversions": ["v√°s√°rl√°s", "konverzi", "conversion", "purchase", "order", "sale"],
    
    # Conversion value
    "conversion_value": ["konverzi√≥s √©rt√©k", "√©rt√©k", "revenue", "value", "bev√©tel", "sales value"],
    
    # Impressions
    "impressions": ["megjelen√©s", "impression", "views", "display"],
    
    # Clicks
    "clicks": ["kattint√°s", "click", "link click", "interakci"],
    
    # CTR
    "ctr_percent": ["ctr", "√°tkattint√°si"],
    
    # CPC
    "cpc": ["cpc", "cost per click", "k√∂lts√©g/kattint√°s"],
    
    # CPA
    "cpa": ["cpa", "k√∂lts√©g/konv", "cost per acquisition", "cost per conversion", "eredm√©nyen", "acquisition cost"],
    
    # ROAS
    "roas": ["roas", "hirdet√©smegt√©r√ºl√©s", "return on ad spend", "megt√©r√ºl√©s"],
    
    # Reach
    "reach": ["el√©r√©s", "reach", "unique reach"],
    
    # Frequency
    "frequency": ["gyakoris√°g", "frequency", "avg frequency"],
    
    # Campaign name
    "campaign_name": ["kamp√°ny", "campaign"],
    
    # Campaign status
    "campaign_status": ["st√°tusz", "status", "state", "enabled", "active"],
    
    # Platform
    "platform": ["platform", "csatorna", "channel", "channel type"],
    
    # Date
    "date_start": ["kezdete", "start", "from", "report start"],
    "date_end": ["v√©ge", "end", "to", "report end"],
    
    # Add to cart
    "add_to_cart": ["kos√°rba", "add to cart", "cart addition"],
    
    # Video views
    "video_views": ["vide√≥", "video view", "video play", "video watch"],
}

# ============================================================================
# üîß HELPER FUNCTIONS
# ============================================================================

def find_matching_column(csv_column, patterns_dict, threshold=70):
    """Fuzzy match CSV column to unified schema"""
    csv_col_lower = csv_column.lower().strip()
    
    best_match = None
    best_score = 0
    
    for unified_field, patterns in patterns_dict.items():
        for pattern in patterns:
            score = fuzz.partial_ratio(csv_col_lower, pattern.lower())
            if score > best_score:
                best_score = score
                best_match = unified_field
    
    if best_score >= threshold:
        return best_match, best_score
    return None, best_score


def intelligently_map_columns(df_columns):
    """Create mapping from CSV columns to unified schema"""
    mapping = {}
    unmapped = []
    
    for col in df_columns:
        matched_field, score = find_matching_column(col, COLUMN_PATTERNS)
        if matched_field:
            mapping[col] = matched_field
        else:
            unmapped.append(col)
    
    return mapping, unmapped


def parse_numeric_value(val):
    """Parse Hungarian-formatted numbers"""
    if pd.isna(val) or val == '' or val == '‚Äì' or val == '--':
        return np.nan
    
    if isinstance(val, (int, float)):
        return float(val)
    
    val_str = str(val).strip()
    val_str = val_str.replace(" ", "").replace(",", ".")
    
    try:
        return float(val_str)
    except:
        return np.nan


def parse_percentage(val):
    """Parse percentage values"""
    if pd.isna(val) or val == '' or val == '‚Äì':
        return np.nan
    
    val_str = str(val).strip()
    val_str = val_str.replace("%", "").replace(",", ".")
    
    try:
        pct = float(val_str)
        # If value is > 1, assume it's already a percentage (not decimal)
        return pct if pct <= 100 else pct / 100
    except:
        return np.nan


def parse_date(val):
    """Parse date values"""
    if pd.isna(val):
        return None
    
    date_formats = [
        "%Y-%m-%d",
        "%Y.%m.%d",
        "%d.%m.%Y",
        "%d-%m-%Y",
        "%m/%d/%Y",
    ]
    
    for fmt in date_formats:
        try:
            return pd.to_datetime(val, format=fmt)
        except:
            continue
    
    try:
        return pd.to_datetime(val)
    except:
        return None


def normalize_data(df, mapping, user_adjustments=None):
    """Normalize and clean imported data"""
    
    # Apply user adjustments if provided
    if user_adjustments:
        mapping = {**mapping, **user_adjustments}
    
    # Create unified dataframe
    normalized_df = pd.DataFrame()
    
    for csv_col, unified_col in mapping.items():
        if csv_col not in df.columns:
            continue
        
        # Get the field info
        field_info = None
        for section in [UNIFIED_SCHEMA["mandatory"], UNIFIED_SCHEMA["recommended"], UNIFIED_SCHEMA["optional"]]:
            for field in section:
                if field[0] == unified_col:
                    field_info = field
                    break
        
        if not field_info:
            continue
        
        field_name, field_type, _ = field_info
        raw_data = df[csv_col]
        
        # Apply type-specific parsing
        if field_type == "float":
            normalized_df[field_name] = raw_data.apply(parse_numeric_value)
        elif field_type == "int":
            normalized_df[field_name] = raw_data.apply(lambda x: int(parse_numeric_value(x)) if not pd.isna(parse_numeric_value(x)) else np.nan)
        elif field_type == "date":
            normalized_df[field_name] = raw_data.apply(parse_date)
        elif field_type == "string":
            normalized_df[field_name] = raw_data.astype(str)
        else:
            normalized_df[field_name] = raw_data
    
    # Calculate missing metrics
    if "spend" in normalized_df.columns and "conversion_value" in normalized_df.columns:
        if "roas" not in normalized_df.columns:
            normalized_df["roas"] = normalized_df["conversion_value"] / normalized_df["spend"]
            normalized_df["roas"] = normalized_df["roas"].replace([np.inf, -np.inf], np.nan)
    
    if "spend" in normalized_df.columns and "conversions" in normalized_df.columns:
        if "cpa" not in normalized_df.columns:
            normalized_df["cpa"] = normalized_df["spend"] / normalized_df["conversions"]
            normalized_df["cpa"] = normalized_df["cpa"].replace([np.inf, -np.inf], np.nan)
    
    if "clicks" in normalized_df.columns and "impressions" in normalized_df.columns:
        if "ctr_percent" not in normalized_df.columns:
            normalized_df["ctr_percent"] = (normalized_df["clicks"] / normalized_df["impressions"] * 100)
            normalized_df["ctr_percent"] = normalized_df["ctr_percent"].replace([np.inf, -np.inf], np.nan)
    
    if "spend" in normalized_df.columns and "clicks" in normalized_df.columns:
        if "cpc" not in normalized_df.columns:
            normalized_df["cpc"] = normalized_df["spend"] / normalized_df["clicks"]
            normalized_df["cpc"] = normalized_df["cpc"].replace([np.inf, -np.inf], np.nan)
    
    return normalized_df


def validate_data(df):
    """Validate normalized data"""
    issues = []
    
    # Check mandatory fields
    mandatory_fields = [f[0] for f in UNIFIED_SCHEMA["mandatory"]]
    for field in mandatory_fields:
        if field not in df.columns:
            issues.append(f"‚ùå Hi√°nyzik: {field}")
        elif df[field].isna().sum() > len(df) * 0.5:
            issues.append(f"‚ö†Ô∏è T√∫l sok hi√°nyzik: {field} ({df[field].isna().sum()} / {len(df)})")
    
    # Check value ranges
    if "roas" in df.columns:
        invalid_roas = df[(df["roas"] < 0) | (df["roas"] > 100)].shape[0]
        if invalid_roas > 0:
            issues.append(f"‚ö†Ô∏è √ârv√©nytelen ROAS √©rt√©kek: {invalid_roas}")
    
    if "cpa" in df.columns:
        invalid_cpa = df[(df["cpa"] < 0)].shape[0]
        if invalid_cpa > 0:
            issues.append(f"‚ö†Ô∏è Negat√≠v CPA √©rt√©kek: {invalid_cpa}")
    
    return issues


# ============================================================================
# üé® STREAMLIT UI
# ============================================================================

st.title("üéØ HYPER - Marketing Campaign Analyzer")
st.markdown("### F√°zis 1: Intelligens CSV Importer")

# Initialize session state
if "uploaded_data" not in st.session_state:
    st.session_state.uploaded_data = None
if "mapping" not in st.session_state:
    st.session_state.mapping = {}
if "normalized_data" not in st.session_state:
    st.session_state.normalized_data = None

# ============================================================================
# TAB 1: UPLOAD & MAPPING
# ============================================================================

tab1, tab2, tab3, tab4 = st.tabs(["üì• Felt√∂lt√©s & Mapping", "‚úÖ Valid√°ci√≥", "üìä El≈ën√©zet", "üíæ Ment√©s"])

with tab1:
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("1Ô∏è‚É£ CSV/Excel Felt√∂lt√©s")
        uploaded_file = st.file_uploader(
            "V√°lassz CSV vagy Excel f√°jlt",
            type=["csv", "xlsx", "xls"],
            help="Facebook, Google Ads vagy TikTok export"
        )
    
    with col2:
        st.subheader("‚ÑπÔ∏è T√°mogatott form√°tumok")
        st.markdown("""
        - ‚úÖ Facebook Ads Manager
        - ‚úÖ Google Ads
        - ‚è≥ TikTok (hamarosan)
        """)
    
    if uploaded_file:
        try:
            # Load file
            if uploaded_file.name.endswith('.csv'):
                raw_df = pd.read_csv(uploaded_file, encoding='utf-8-sig')
            else:
                raw_df = pd.read_excel(uploaded_file)
            
            st.session_state.uploaded_data = raw_df
            
            st.success(f"‚úÖ Bet√∂ltve: {uploaded_file.name}")
            st.info(f"üìä Sorok: {len(raw_df)}, Oszlopok: {len(raw_df.columns)}")
            
            # ====================================================================
            # INTELLIGENT COLUMN MAPPING
            # ====================================================================
            
            st.subheader("2Ô∏è‚É£ Automata Oszlop Felismer√©s")
            
            initial_mapping, unmapped = intelligently_map_columns(raw_df.columns)
            
            st.session_state.mapping = initial_mapping
            
            st.markdown("#### üîÑ Automatikusan felismert oszlopok:")
            
            # Show mapped columns
            mapped_cols = st.expander("‚úÖ Lek√©pezett oszlopok", expanded=True)
            with mapped_cols:
                mapping_display = []
                for csv_col, unified_col in sorted(initial_mapping.items()):
                    mapping_display.append({
                        "CSV Oszlop": csv_col,
                        "Unified Field": unified_col,
                    })
                
                if mapping_display:
                    st.dataframe(pd.DataFrame(mapping_display), use_container_width=True)
                else:
                    st.warning("Nincs automata felismer√©s :(")
            
            # Show unmapped columns
            if unmapped:
                unmapped_cols = st.expander(f"‚ö†Ô∏è Felismeretlen oszlopok ({len(unmapped)})")
                with unmapped_cols:
                    st.warning(f"A k√∂vetkez≈ë oszlopok nem ker√ºltek besorol√°sra:")
                    for col in unmapped:
                        st.text(f"‚Ä¢ {col}")
            
            # Preview
            st.subheader("üìã Adatok El≈ën√©zete (Raw)")
            st.dataframe(raw_df.head(5), use_container_width=True)
            
        except Exception as e:
            st.error(f"‚ùå Hiba a f√°jl felt√∂lt√©sekor: {str(e)}")

with tab2:
    if st.session_state.uploaded_data is not None:
        st.subheader("‚úÖ Adatok Normaliz√°l√°sa & Valid√°l√°sa")
        
        try:
            # Normalize
            normalized_df = normalize_data(
                st.session_state.uploaded_data,
                st.session_state.mapping
            )
            st.session_state.normalized_data = normalized_df
            
            # Validate
            validation_issues = validate_data(normalized_df)
            
            if validation_issues:
                st.warning("### ‚ö†Ô∏è Valid√°ci√≥s Figyelmeztet√©sek")
                for issue in validation_issues:
                    st.warning(issue)
            else:
                st.success("### ‚úÖ Minden OK! Az adatok k√©szen √°llnak.")
            
            st.info(f"**Normaliz√°lt adatok**: {len(normalized_df)} sor √ó {len(normalized_df.columns)} oszlop")
        except Exception as e:
            st.error(f"‚ùå Hiba a normaliz√°l√°s sor√°n: {str(e)}")
    else:
        st.info("El≈ësz√∂r t√∂ltsd fel az adatokat a 'üì• Felt√∂lt√©s & Mapping' f√ºl√∂n!")

with tab3:
    if st.session_state.normalized_data is not None:
        st.subheader("üìä Normaliz√°lt Adatok El≈ën√©zete")
        
        try:
            # Show statistics
            col1, col2, col3 = st.columns(3)
            
            df = st.session_state.normalized_data
            
            with col1:
                if "spend" in df.columns:
                    total_spend = df["spend"].sum()
                    st.metric("üí∞ Teljes K√∂lts√©g", f"{total_spend:,.0f} HUF")
            
            with col2:
                if "conversion_value" in df.columns:
                    total_value = df["conversion_value"].sum()
                    st.metric("üíµ Konverzi√≥s √ârt√©k", f"{total_value:,.0f} HUF")
            
            with col3:
                if "roas" in df.columns:
                    avg_roas = df["roas"].mean()
                    st.metric("üìà √Åtlag ROAS", f"{avg_roas:.2f}")
            
            # Platform distribution
            if "platform" in df.columns:
                st.subheader("Platform Megoszl√°s")
                platform_dist = df["platform"].value_counts()
                st.bar_chart(platform_dist)
            
            # Data table
            st.subheader("Adatok T√°bl√°zat")
            st.dataframe(df, use_container_width=True)
        except Exception as e:
            st.error(f"‚ùå Hiba az el≈ën√©zet sor√°n: {str(e)}")
    else:
        st.info("El≈ësz√∂r t√∂ltsd fel az adatokat a 'üì• Felt√∂lt√©s & Mapping' f√ºl√∂n!")

with tab4:
    if st.session_state.normalized_data is not None:
        st.subheader("üíæ Adatok Export√°l√°sa")
        
        try:
            df = st.session_state.normalized_data
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Export as CSV
                csv = df.to_csv(index=False)
                st.download_button(
                    label="üì• CSV let√∂lt√©s",
                    data=csv,
                    file_name=f"hyper_normalized_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
            
            with col2:
                # Export as Excel
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Campaigns')
                
                st.download_button(
                    label="üì• Excel let√∂lt√©s",
                    data=buffer.getvalue(),
                    file_name=f"hyper_normalized_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        except Exception as e:
            st.error(f"‚ùå Hiba az export√°l√°s sor√°n: {str(e)}")
    else:
        st.info("El≈ësz√∂r t√∂ltsd fel az adatokat a 'üì• Felt√∂lt√©s & Mapping' f√ºl√∂n!")

# ============================================================================
# FOOTER
# ============================================================================

st.divider()
st.markdown("""
---
**HYPER App v3.0** | Neuromarketing ROAS Predictor
- ‚úÖ F√°zis 1: CSV Importer & Intelligent Mapper
- ‚è≥ F√°zis 2: Creative Analyzer (GPT4V)
- ‚è≥ F√°zis 3: Live Channel Integration (API)
""")
