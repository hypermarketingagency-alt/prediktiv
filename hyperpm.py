import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime

try:
    from thefuzz import fuzz
except ImportError:
    st.error("Hi√°nyzik: pip install thefuzz python-Levenshtein")
    st.stop()

import io

# ============================================================================
# üé® HYPER App - Neuromarketing ROAS Predictor v3.1
# F√ÅZIS 1: CSV Importer & Intelligent Mapper (Facebook-ra optimaliz√°lva)
# ============================================================================

st.set_page_config(
    page_title="HYPER - Marketing Predictor",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# üìä CONFIGURATION & MAPPINGS
# ============================================================================

UNIFIED_SCHEMA = {
    "mandatory": [
        ("date_start", "date", "Jelent√©s kezdete (d√°tum)"),
        ("date_end", "date", "Jelent√©s v√©ge (d√°tum)"),
        ("campaign_name", "string", "Kamp√°ny neve"),
        ("platform", "string", "Platform (Facebook/Google Ads/TikTok)"),
        ("campaign_status", "string", "Kamp√°ny st√°tusza"),
        ("spend", "float", "Elk√∂lt√∂tt √∂sszeg (HUF)"),
        ("conversions", "int", "Konverzi√≥k / V√°s√°rl√°sok"),
        ("conversion_value", "float", "Konverzi√≥s √©rt√©k (HUF)"),
    ],
    "recommended": [
        ("impressions", "int", "Megjelen√©sek"),
        ("clicks", "int", "Kattint√°sok / Interakci√≥k"),
        ("ctr_percent", "float", "CTR (%)"),
        ("cpc", "float", "CPC (HUF)"),
        ("cpa", "float", "CPA (HUF)"),
        ("roas", "float", "ROAS"),
        ("reach", "int", "El√©r√©s"),
        ("frequency", "float", "Gyakoris√°g"),
        ("ad_group_name", "string", "Ad Set / Ad Group neve"),
        ("budget_type", "string", "K√∂lts√©gkeret t√≠pusa"),
        ("budget_allocated", "float", "K√∂lts√©gkeret (HUF)"),
    ],
    "optional": [
        ("add_to_cart", "int", "Kos√°rba helyez√©sek"),
        ("video_views", "int", "Vide√≥ megtekint√©sek"),
        ("engagement", "int", "Engagement"),
        ("conversion_type", "string", "Konverzi√≥ t√≠pusa"),
        ("notes", "string", "Megjegyz√©sek"),
    ]
}

# Fuzzy matching patterns ‚Äì kamp√°ny n√©v √©s st√°tusz sz√©tv√°lasztva
COLUMN_PATTERNS = {
    # Spend
    "spend": ["elk√∂lt√∂tt √∂sszeg", "k√∂lts√©g", "spend", "amount spent"],

    # Kamp√°ny N√âV
    "campaign_name": ["kamp√°ny neve", "campaign", "campaign name"],

    # Kamp√°ny ST√ÅTUSZ
    "campaign_status": ["kamp√°ny teljes√≠t√©se", "√°llapot", "status", "state", "active", "completed"],

    # D√°tumok
    "date_start": ["jelent√©s kezdete", "start date", "from"],
    "date_end": ["jelent√©s v√©ge", "end date", "to"],

    # Konverzi√≥k
    "conversions": ["v√°s√°rl√°sok", "konverzi√≥k", "purchases", "orders"],
    "conversion_value": [
        "v√°s√°rl√°sok konverzi√≥s √©rt√©ke",
        "kos√°rba helyez√©sek konverzi√≥s √©rt√©ke",
        "konverzi√≥s √©rt√©k",
        "purchase value",
        "conversion value",
        "revenue",
        "bev√©tel"
    ],

    # Impressions
    "impressions": ["megjelen√©sek", "impressions"],

    # Clicks
    "clicks": ["kattint√°s", "clicks", "link click", "interakci√≥"],

    # CTR
    "ctr_percent": ["ctr", "√°tkattint√°si ar√°ny"],

    # CPC
    "cpc": ["cpc", "cost per click", "kattint√°si k√∂lts√©g", "cpc (√∂sszes)"],

    # CPA
    "cpa": ["eredm√©nyenk√©nti k√∂lts√©g", "cpa", "k√∂lts√©g/konv", "cost per conversion"],

    # ROAS
    "roas": ["v√°s√°rl√°si hirdet√©smegt√©r√ºl√©s", "roas", "hirdet√©smegt√©r√ºl√©s"],

    # Reach
    "reach": ["el√©r√©s", "reach"],

    # Frequency
    "frequency": ["gyakoris√°g", "frequency"],

    # Add to cart
    "add_to_cart": ["kos√°rba helyez√©sek", "add to cart"],

    # Platform ‚Äì ezt ink√°bb k√©s≈ëbb √°ll√≠tjuk be k√©zzel
}

# ============================================================================
# üîß HELPER FUNCTIONS
# ============================================================================

def find_matching_column(csv_column, patterns_dict, threshold=70):
    csv_col_lower = csv_column.lower().strip()
    best_match = None
    best_score = 0

    for unified_field, patterns in patterns_dict.items():
        for pattern in patterns:
            score = fuzz.partial_ratio(csv_col_lower, pattern.lower())
            if score > best_score:
                best_score = score
                best_match = unified_field

    if best_score >= threshold:
        return best_match, best_score
    return None, best_score


def intelligently_map_columns(df_columns):
    mapping = {}
    unmapped = []

    for col in df_columns:
        matched_field, score = find_matching_column(col, COLUMN_PATTERNS)
        if matched_field:
            mapping[col] = matched_field
        else:
            unmapped.append(col)

    return mapping, unmapped


def parse_numeric_value(val):
    if pd.isna(val) or val == "" or val == "‚Äì" or val == "--":
        return np.nan
    if isinstance(val, (int, float)):
        return float(val)
    val_str = str(val).strip()
    val_str = val_str.replace(" ", "").replace(",", ".")
    try:
        return float(val_str)
    except Exception:
        return np.nan


def parse_date(val):
    if pd.isna(val):
        return None
    date_formats = [
        "%Y-%m-%d",
        "%Y.%m.%d",
        "%d.%m.%Y",
        "%d-%m-%Y",
        "%m/%d/%Y",
    ]
    for fmt in date_formats:
        try:
            return pd.to_datetime(val, format=fmt)
        except Exception:
            continue
    try:
        return pd.to_datetime(val)
    except Exception:
        return None


def normalize_data(df, mapping, user_adjustments=None, platform_hint=None):
    if user_adjustments:
        mapping = {**mapping, **user_adjustments}

    normalized_df = pd.DataFrame()

    for csv_col, unified_col in mapping.items():
        if csv_col not in df.columns:
            continue

        field_info = None
        for section in [
            UNIFIED_SCHEMA["mandatory"],
            UNIFIED_SCHEMA["recommended"],
            UNIFIED_SCHEMA["optional"],
        ]:
            for field in section:
                if field[0] == unified_col:
                    field_info = field
                    break

        if not field_info:
            continue

        field_name, field_type, _ = field_info
        raw_data = df[csv_col]

        if field_type == "float":
            normalized_df[field_name] = raw_data.apply(parse_numeric_value)
        elif field_type == "int":
            normalized_df[field_name] = raw_data.apply(
                lambda x: int(parse_numeric_value(x))
                if not pd.isna(parse_numeric_value(x))
                else np.nan
            )
        elif field_type == "date":
            normalized_df[field_name] = raw_data.apply(parse_date)
        elif field_type == "string":
            normalized_df[field_name] = raw_data.astype(str)
        else:
            normalized_df[field_name] = raw_data

    # Ha nincs date_end, t√∂lts√ºk fel a date_starttal
    if "date_start" in normalized_df.columns and "date_end" not in normalized_df.columns:
        normalized_df["date_end"] = normalized_df["date_start"]

    # Platform automata be√°ll√≠t√°s (pl. Facebook)
    if "platform" not in normalized_df.columns:
        if platform_hint:
            normalized_df["platform"] = platform_hint
        else:
            normalized_df["platform"] = "Unknown"

    # Sz√°molt metrik√°k
    if "spend" in normalized_df.columns and "conversion_value" in normalized_df.columns:
        if "roas" not in normalized_df.columns:
            normalized_df["roas"] = (
                normalized_df["conversion_value"] / normalized_df["spend"]
            )
            normalized_df["roas"] = normalized_df["roas"].replace(
                [np.inf, -np.inf], np.nan
            )

    if "spend" in normalized_df.columns and "conversions" in normalized_df.columns:
        if "cpa" not in normalized_df.columns:
            normalized_df["cpa"] = (
                normalized_df["spend"] / normalized_df["conversions"]
            )
            normalized_df["cpa"] = normalized_df["cpa"].replace(
                [np.inf, -np.inf], np.nan
            )

    if "clicks" in normalized_df.columns and "impressions" in normalized_df.columns:
        if "ctr_percent" not in normalized_df.columns:
            normalized_df["ctr_percent"] = (
                normalized_df["clicks"] / normalized_df["impressions"] * 100
            )
            normalized_df["ctr_percent"] = normalized_df["ctr_percent"].replace(
                [np.inf, -np.inf], np.nan
            )

    if "spend" in normalized_df.columns and "clicks" in normalized_df.columns:
        if "cpc" not in normalized_df.columns:
            normalized_df["cpc"] = normalized_df["spend"] / normalized_df["clicks"]
            normalized_df["cpc"] = normalized_df["cpc"].replace(
                [np.inf, -np.inf], np.nan
            )

    return normalized_df


def validate_data(df):
    issues = []
    mandatory_fields = [f[0] for f in UNIFIED_SCHEMA["mandatory"]]
    for field in mandatory_fields:
        if field not in df.columns:
            issues.append(f"‚ùå Hi√°nyzik: {field}")
        elif df[field].isna().sum() > len(df) * 0.5:
            issues.append(
                f"‚ö†Ô∏è T√∫l sok hi√°nyzik: {field} ({df[field].isna().sum()} / {len(df)})"
            )

    if "roas" in df.columns:
        invalid_roas = df[(df["roas"] < 0) | (df["roas"] > 100)].shape[0]
        if invalid_roas > 0:
            issues.append(f"‚ö†Ô∏è √ârv√©nytelen ROAS √©rt√©kek: {invalid_roas}")

    if "cpa" in df.columns:
        invalid_cpa = df[(df["cpa"] < 0)].shape[0]
        if invalid_cpa > 0:
            issues.append(f"‚ö†Ô∏è Negat√≠v CPA √©rt√©kek: {invalid_cpa}")

    return issues

# ============================================================================
# üé® STREAMLIT UI
# ============================================================================

st.title("üéØ HYPER - Marketing Campaign Analyzer")
st.markdown("## F√°zis 1: Intelligens CSV Importer")

if "uploaded_data" not in st.session_state:
    st.session_state.uploaded_data = None
if "mapping" not in st.session_state:
    st.session_state.mapping = {}
if "normalized_data" not in st.session_state:
    st.session_state.normalized_data = None

tab1, tab2, tab3, tab4 = st.tabs(
    ["üì• Felt√∂lt√©s & Mapping", "‚úÖ Valid√°ci√≥", "üìä El≈ën√©zet", "üíæ Ment√©s"]
)

# ---------------------------------------------------------------------------
# TAB 1 ‚Äì Felt√∂lt√©s & Mapping
# ---------------------------------------------------------------------------
with tab1:
    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("1Ô∏è‚É£ CSV/Excel Felt√∂lt√©s")
        uploaded_file = st.file_uploader(
            "V√°lassz CSV vagy Excel f√°jlt",
            type=["csv", "xlsx", "xls"],
            help="Facebook, Google Ads vagy TikTok export",
        )

    with col2:
        st.subheader("‚ÑπÔ∏è T√°mogatott form√°tumok")
        st.markdown(
            """
        - ‚úÖ Facebook Ads Manager
        - ‚úÖ Google Ads
        - ‚è≥ TikTok (hamarosan)
        """
        )

    platform_hint = st.selectbox(
        "Melyik platformr√≥l sz√°rmazik ez a f√°jl?",
        ["Facebook", "Google Ads", "TikTok", "Ismeretlen"],
        index=0,
        help="Ez beker√ºl a 'platform' oszlopba, ha a CSV-ben nincs ilyen mez≈ë.",
    )

    if uploaded_file:
        try:
            if uploaded_file.name.endswith(".csv"):
                raw_df = pd.read_csv(uploaded_file, encoding="utf-8-sig")
            else:
                raw_df = pd.read_excel(uploaded_file)

            st.session_state.uploaded_data = raw_df

            st.success(f"‚úÖ Bet√∂ltve: {uploaded_file.name}")
            st.info(f"üìä Sorok: {len(raw_df)}, Oszlopok: {len(raw_df.columns)}")

            st.subheader("2Ô∏è‚É£ Automata Oszlop Felismer√©s")

            initial_mapping, unmapped = intelligently_map_columns(raw_df.columns)
            st.session_state.mapping = initial_mapping

            st.markdown("#### üîÑ Automatikusan felismert oszlopok:")
            mapped_cols = st.expander("‚úÖ Lek√©pezett oszlopok", expanded=True)
            with mapped_cols:
                mapping_display = [
                    {"CSV Oszlop": csv_col, "Unified Field": unified_col}
                    for csv_col, unified_col in sorted(initial_mapping.items())
                ]
                if mapping_display:
                    st.dataframe(
                        pd.DataFrame(mapping_display), use_container_width=True
                    )
                else:
                    st.warning("Nincs automata felismer√©s :(")

            if unmapped:
                unmapped_cols = st.expander(
                    f"‚ö†Ô∏è Felismeretlen oszlopok ({len(unmapped)})"
                )
                with unmapped_cols:
                    st.warning("A k√∂vetkez≈ë oszlopok nem ker√ºltek besorol√°sra:")
                    for col in unmapped:
                        st.text(f"‚Ä¢ {col}")

            st.subheader("3Ô∏è‚É£ Manu√°lis Korrekci√≥ (opcion√°lis)")
            st.markdown(
                "Ha valamelyik mez≈ë rossz helyre ker√ºlt (pl. kamp√°ny n√©v vs. st√°tusz), itt tudod jav√≠tani."
            )

            manual_corrections = {}
            all_fields = ["--Nincs--"]
            for section in [
                UNIFIED_SCHEMA["mandatory"],
                UNIFIED_SCHEMA["recommended"],
                UNIFIED_SCHEMA["optional"],
            ]:
                for field in section:
                    all_fields.append(field[0])

            with st.expander("Manu√°lis mapping szerkeszt√©se"):
                for csv_col in raw_df.columns:
                    current_mapping = initial_mapping.get(csv_col, "--Nincs--")
                    new_mapping = st.selectbox(
                        f"{csv_col}",
                        all_fields,
                        index=all_fields.index(current_mapping)
                        if current_mapping in all_fields
                        else 0,
                        key=f"manual_map_{csv_col}",
                    )
                    if new_mapping != "--Nincs--":
                        manual_corrections[csv_col] = new_mapping

            if manual_corrections:
                st.session_state.mapping.update(manual_corrections)

            st.subheader("üìã Adatok El≈ën√©zete (RAW)")
            st.dataframe(raw_df.head(10), use_container_width=True)

            st.session_state.platform_hint = (
                platform_hint if platform_hint != "Ismeretlen" else None
            )

        except Exception as e:
            st.error(f"‚ùå Hiba a f√°jl feldolgoz√°sakor: {str(e)}")

# ---------------------------------------------------------------------------
# TAB 2 ‚Äì Valid√°ci√≥
# ---------------------------------------------------------------------------
with tab2:
    if st.session_state.uploaded_data is not None:
        st.subheader("‚úÖ Adatok Normaliz√°l√°sa & Valid√°l√°sa")
        try:
            normalized_df = normalize_data(
                st.session_state.uploaded_data,
                st.session_state.mapping,
                platform_hint=getattr(st.session_state, "platform_hint", None),
            )
            st.session_state.normalized_data = normalized_df

            validation_issues = validate_data(normalized_df)

            if validation_issues:
                st.warning("### ‚ö†Ô∏è Valid√°ci√≥s Figyelmeztet√©sek")
                for issue in validation_issues:
                    st.warning(issue)
            else:
                st.success("### ‚úÖ Minden OK! Az adatok k√©szen √°llnak.")

            st.info(
                f"Normaliz√°lt adatok: {len(normalized_df)} sor √ó {len(normalized_df.columns)} oszlop"
            )
        except Exception as e:
            st.error(f"‚ùå Hiba a normaliz√°l√°s sor√°n: {str(e)}")
    else:
        st.info("El≈ësz√∂r t√∂ltsd fel az adatokat a 'üì• Felt√∂lt√©s & Mapping' f√ºl√∂n!")

# ---------------------------------------------------------------------------
# TAB 3 ‚Äì El≈ën√©zet
# ---------------------------------------------------------------------------
with tab3:
    if st.session_state.normalized_data is not None:
        st.subheader("üìä Normaliz√°lt Adatok El≈ën√©zete")
        try:
            df = st.session_state.normalized_data

            col1, col2, col3 = st.columns(3)
            with col1:
                if "spend" in df.columns:
                    st.metric("üí∞ Teljes K√∂lts√©g", f"{df['spend'].sum():,.0f} HUF")
            with col2:
                if "conversion_value" in df.columns:
                    st.metric(
                        "üíµ Konverzi√≥s √ârt√©k",
                        f"{df['conversion_value'].sum():,.0f} HUF",
                    )
            with col3:
                if "roas" in df.columns:
                    st.metric("üìà √Åtlag ROAS", f"{df['roas'].mean():.2f}")

            st.subheader("Adatok T√°bl√°zat")
            st.dataframe(df, use_container_width=True)
        except Exception as e:
            st.error(f"‚ùå Hiba az el≈ën√©zet sor√°n: {str(e)}")
    else:
        st.info("El≈ësz√∂r t√∂ltsd fel √©s normaliz√°ld az adatokat.")

# ---------------------------------------------------------------------------
# TAB 4 ‚Äì Ment√©s
# ---------------------------------------------------------------------------
with tab4:
    if st.session_state.normalized_data is not None:
        st.subheader("üíæ Adatok Export√°l√°sa")
        df = st.session_state.normalized_data

        col1, col2 = st.columns(2)
        with col1:
            csv = df.to_csv(index=False)
            st.download_button(
                "üì• CSV let√∂lt√©s",
                data=csv,
                file_name=f"hyper_normalized_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
            )
        with col2:
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
                df.to_excel(writer, index=False, sheet_name="Campaigns")
            st.download_button(
                "üì• Excel let√∂lt√©s",
                data=buffer.getvalue(),
                file_name=f"hyper_normalized_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
    else:
        st.info("El≈ësz√∂r t√∂ltsd fel √©s normaliz√°ld az adatokat.")

st.divider()
st.markdown(
    """
**HYPER App v3.1** | Neuromarketing ROAS Predictor  
F√°zis 1 k√©sz ‚Äì j√∂het a F√°zis 2 (Creative Analyzer + ML modell).
"""
)
